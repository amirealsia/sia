#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì»¨í…ì¸  ì•ˆì „ í•„í„°ë§ ëª¨ë“ˆ
ëª¨ë“  í…ìŠ¤íŠ¸ ì…ë ¥/ì¶œë ¥ì„ ê²€ì¦í•˜ì—¬ ë¶€ì ì ˆí•œ ë‚´ìš© ì°¨ë‹¨
"""

import os
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import re

BASE_DIR = Path(__file__).resolve().parents[1]
load_dotenv(BASE_DIR / "secrets" / ".env")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# í•œê¸€ ë¶€ì ì ˆ í‚¤ì›Œë“œ ë¸”ë™ë¦¬ìŠ¤íŠ¸
KOREAN_BLACKLIST = [
    # ì„±ì  í‘œí˜„
    "ì„¹ìŠ¤", "ì•¼ë™", "ì•¼í•œ", "ìŒë€", "í¬ë¥´ë…¸", "19ê¸ˆ", "ì„±ì¸", "ëª¸ë§¤",
    "ê°€ìŠ´", "ì—‰ë©ì´", "ì„¹ì‹œ", "ë²—ë‹¤", "ì•Œëª¸", "ëˆ„ë“œ", "ì•¼ì‚¬",
    # í˜ì˜¤ í‘œí˜„
    "ì£½ì—¬", "ì£½ì´ë‹¤", "ìì‚´", "ì‚´ì¸", "í­ë ¥", "ë•Œë¦¬ë‹¤", "ê°•ê°„",
    # ë¹„í•˜ í‘œí˜„
    "ì‹«ì–´", "ë”ëŸ¬ìš´", "ì¶”í•œ", "ëª»ìƒê¸´", "ì“°ë ˆê¸°", "ë³‘ì‹ "
]

# ì˜ì–´ ë¶€ì ì ˆ í‚¤ì›Œë“œ ë¸”ë™ë¦¬ìŠ¤íŠ¸
ENGLISH_BLACKLIST = [
    # Sexual content
    "sex", "porn", "nude", "naked", "nsfw", "xxx", "sexy", "erotic",
    "breast", "boob", "ass", "penis", "vagina", "dick", "pussy",
    "fuck", "fucking", "horny", "orgasm", "masturbate",
    # Violence
    "kill", "murder", "death", "suicide", "rape", "violence", "gun", "blood",
    # Hate speech
    "hate", "ugly", "disgusting", "trash", "stupid", "idiot"
]

def check_text_safety(text, strict=True):
    """
    í…ìŠ¤íŠ¸ì˜ ì•ˆì „ì„±ì„ OpenAI Moderation APIë¡œ ê²€ì‚¬

    Args:
        text: ê²€ì‚¬í•  í…ìŠ¤íŠ¸
        strict: Trueë©´ ì—„ê²©í•œ í•„í„°ë§, Falseë©´ ê´€ëŒ€í•œ í•„í„°ë§

    Returns:
        {
            "safe": bool,
            "reason": str,
            "categories": dict
        }
    """
    if not text or len(text.strip()) == 0:
        return {"safe": True, "reason": "", "categories": {}}

    try:
        # Step 1: ë¸”ë™ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì²´í¬
        text_lower = text.lower()

        for keyword in KOREAN_BLACKLIST + ENGLISH_BLACKLIST:
            if keyword in text_lower:
                return {
                    "safe": False,
                    "reason": f"ë¶€ì ì ˆí•œ í‚¤ì›Œë“œ ê°ì§€: {keyword}",
                    "categories": {"blacklist": True}
                }

        # Step 2: OpenAI Moderation API
        response = client.moderations.create(input=text)
        result = response.results[0]

        # ì¹´í…Œê³ ë¦¬ë³„ ì²´í¬
        flagged_categories = []

        if result.categories.sexual or result.categories.sexual_minors:
            flagged_categories.append("sexual")

        if result.categories.hate or result.categories.hate_threatening:
            flagged_categories.append("hate")

        if result.categories.violence or result.categories.violence_graphic:
            flagged_categories.append("violence")

        if result.categories.harassment or result.categories.harassment_threatening:
            flagged_categories.append("harassment")

        if result.categories.self_harm or result.categories.self_harm_intent:
            flagged_categories.append("self_harm")

        # ì—„ê²© ëª¨ë“œ: í•˜ë‚˜ë¼ë„ ê°ì§€ë˜ë©´ ì°¨ë‹¨
        # ì¼ë°˜ ëª¨ë“œ: ë†’ì€ í™•ë¥ (score > 0.5)ë§Œ ì°¨ë‹¨
        if strict:
            is_safe = len(flagged_categories) == 0
        else:
            is_safe = not result.flagged

        if not is_safe:
            reason = f"ë¶€ì ì ˆí•œ ë‚´ìš© ê°ì§€: {', '.join(flagged_categories)}"
        else:
            reason = ""

        return {
            "safe": is_safe,
            "reason": reason,
            "categories": result.categories.model_dump()
        }

    except Exception as e:
        print(f"âš ï¸ ì»¨í…ì¸  ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì°¨ë‹¨
        return {
            "safe": False,
            "reason": f"ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}",
            "categories": {}
        }

def sanitize_prompt(prompt):
    """
    ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ì—ì„œ ë¶€ì ì ˆí•œ í‚¤ì›Œë“œ ì œê±°

    Args:
        prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸

    Returns:
        ì •ì œëœ í”„ë¡¬í”„íŠ¸
    """
    cleaned = prompt

    # ë¸”ë™ë¦¬ìŠ¤íŠ¸ í‚¤ì›Œë“œ ì œê±°
    for keyword in KOREAN_BLACKLIST + ENGLISH_BLACKLIST:
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        cleaned = pattern.sub("", cleaned)

    # ì¤‘ë³µ ê³µë°± ì œê±°
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    return cleaned

def get_safe_negative_prompt():
    """
    ê°•ë ¥í•œ ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (NSFW ë°©ì§€)

    Returns:
        ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    return """
nsfw, nude, naked, sexual, porn, pornographic, xxx, adult content,
explicit, erotic, provocative, suggestive, revealing clothes,
cleavage, underwear, lingerie, bikini, swimsuit,
violence, gore, blood, injury, weapon, gun, knife,
hate, offensive, disturbing, shocking, inappropriate,
low quality, blurry, bad anatomy, deformed, disfigured,
ugly, disgusting, horror, nightmare, dark, scary
"""

def validate_story_output(story_text):
    """
    ìƒì„±ëœ ìŠ¤í† ë¦¬ì˜ ì•ˆì „ì„± ê²€ì¦

    Args:
        story_text: AIê°€ ìƒì„±í•œ ìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸

    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    result = check_text_safety(story_text, strict=True)

    if not result["safe"]:
        print(f"âš ï¸ ë¶€ì ì ˆí•œ ìŠ¤í† ë¦¬ ìƒì„±ë¨: {result['reason']}")
        print(f"   ì›ë³¸: {story_text[:100]}...")

        # ê¸°ë³¸ ì•ˆì „ ìŠ¤í† ë¦¬ë¡œ ëŒ€ì²´
        return {
            "safe": False,
            "original": story_text,
            "replacement": "ì˜¤ëŠ˜ë„ ìƒˆë¡œìš´ í•˜ë£¨ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤. í‰ë²”í•˜ì§€ë§Œ ì†Œì¤‘í•œ ìˆœê°„ë“¤.",
            "reason": result["reason"]
        }

    return {
        "safe": True,
        "original": story_text,
        "replacement": story_text,
        "reason": ""
    }

def validate_comment_input(comment):
    """
    ì‚¬ìš©ì ëŒ“ê¸€/ì…ë ¥ ê²€ì¦

    Args:
        comment: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸

    Returns:
        ê²€ì¦ ê²°ê³¼
    """
    result = check_text_safety(comment, strict=False)

    if not result["safe"]:
        return {
            "allowed": False,
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ™",
            "reason": result["reason"]
        }

    return {
        "allowed": True,
        "response": None,
        "reason": ""
    }

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
if __name__ == "__main__":
    print("=" * 60)
    print("ì»¨í…ì¸  ì•ˆì „ í•„í„° í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸ 1: ì•ˆì „í•œ í…ìŠ¤íŠ¸
    safe_text = "ì˜¤ëŠ˜ì€ ì¹´í˜ì—ì„œ ì»¤í”¼ë¥¼ ë§ˆì‹œë©° ì±…ì„ ì½ì—ˆì–´ìš”. í‰í™”ë¡œìš´ í•˜ë£¨ì˜€ìŠµë‹ˆë‹¤."
    result = check_text_safety(safe_text)
    print(f"\nâœ… ì•ˆì „í•œ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸:")
    print(f"   ì…ë ¥: {safe_text}")
    print(f"   ê²°ê³¼: {'í†µê³¼' if result['safe'] else 'ì°¨ë‹¨'}")

    # í…ŒìŠ¤íŠ¸ 2: ë¶€ì ì ˆí•œ í…ìŠ¤íŠ¸
    unsafe_text = "ì´ í…ìŠ¤íŠ¸ëŠ” ë¶€ì ì ˆí•œ ì„±ì  í‘œí˜„ì„ í¬í•¨í•©ë‹ˆë‹¤"
    result = check_text_safety(unsafe_text)
    print(f"\nâŒ ë¶€ì ì ˆí•œ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸:")
    print(f"   ì…ë ¥: {unsafe_text}")
    print(f"   ê²°ê³¼: {'í†µê³¼' if result['safe'] else 'ì°¨ë‹¨'}")
    print(f"   ì‚¬ìœ : {result['reason']}")

    # í…ŒìŠ¤íŠ¸ 3: í”„ë¡¬í”„íŠ¸ ì •ì œ
    dirty_prompt = "a beautiful girl, sexy pose, nude, in bedroom"
    clean = sanitize_prompt(dirty_prompt)
    print(f"\nğŸ§¹ í”„ë¡¬í”„íŠ¸ ì •ì œ í…ŒìŠ¤íŠ¸:")
    print(f"   ì›ë³¸: {dirty_prompt}")
    print(f"   ì •ì œ: {clean}")

    # í…ŒìŠ¤íŠ¸ 4: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
    negative = get_safe_negative_prompt()
    print(f"\nğŸ›¡ï¸ ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸:")
    print(f"   {negative[:100]}...")
